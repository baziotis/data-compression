# data-compression
Various data compression techniques and algorithms

# Shannon Entropy
There once was a very smart dude in the CS world and specifically, in the [Information Theory](https://en.wikipedia.org/wiki/Information_theory) field, named [Claude Shannon](https://www.itsoc.org/about/shannon). Among other things, he found that for a data set, there is a smallest possible number of bits needed to represent the entire data set. He called this number entropy.
Entropy is defined by the following formula:
![Entropy Formula](/readme_images/entropy_formula.png)

where probability (or pi) is how many times i occurs in the data set (or string) divided by the length of the set (or string) or: <b>pi = count(i) / len(s)</b>
